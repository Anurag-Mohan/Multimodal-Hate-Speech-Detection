{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-08T10:51:41.286011Z",
     "iopub.status.busy": "2026-02-08T10:51:41.285678Z",
     "iopub.status.idle": "2026-02-08T11:01:31.156695Z",
     "shell.execute_reply": "2026-02-08T11:01:31.155902Z",
     "shell.execute_reply.started": "2026-02-08T10:51:41.285984Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ INITIALIZING TRAINING PIPELINE...\n",
      "   ‚úÖ Data Found: /kaggle/input/hatefulmemesproject/facebook/data/train.jsonl\n",
      "   ‚úÖ GloVe Found: glove.6B.300d.txt\n",
      "   üöÄ Accelerator: cuda (GPUs: 2)\n",
      "üöß Building Vocabulary...\n",
      "üîå Loading GloVe from: glove.6B.300d.txt\n",
      "   ‚úÖ GloVe Loaded: 4793 words matched.\n",
      "‚ö° Dual GPU Activated.\n",
      "\n",
      "=== STAGE 1: PRE-TRAINING (30k) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/4165983151.py:243: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c408629af2b4638b7d4e88ff9a83bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/235 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/4165983151.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STAGE 2: FINE-TUNING ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c888b1b6adc4c32a4c2fb2f797c8461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/4165983151.py:290: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/tmp/ipykernel_55/4165983151.py:303: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Val Acc: 50.00% | Loss: 23.7376\n",
      "   üíæ Saved: 50.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea49d89a82646eb8b7907a6fb5ed846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Val Acc: 50.00% | Loss: 25.7989\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef20a0616504073aee911b172b6e89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Val Acc: 50.00% | Loss: 11.9520\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c60555e3c440f8ac8d0559bbea524d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Val Acc: 50.00% | Loss: 0.9381\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ce232864c734df28f936cc243d279f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Val Acc: 50.00% | Loss: 16.0829\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdbe1b4cc9d463cb0722d2e4204ab90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Val Acc: 50.00% | Loss: 6.0472\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeba8053cc664f4f92873e3d2038f854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Val Acc: 54.60% | Loss: 0.1966\n",
      "   üíæ Saved: 54.60%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc075588c2846f78e22c7fa17fa5fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Val Acc: 50.00% | Loss: 0.8020\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d914ff3fc6449d2a47a06a988c8c866",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Val Acc: 50.00% | Loss: 0.7947\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855d2d12261e472b9f0d8617e62f8530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Val Acc: 51.60% | Loss: 0.2308\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================\n",
    "#  üöÄ HATEFUL MEME DETECTION - KAGGLE ENTERPRISE PIPELINE (FINAL FIX)\n",
    "#  Features: Dual GPU | Mixed Precision | Focal Loss | Auto-GloVe\n",
    "# ==================================================================================\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import collections\n",
    "import nltk\n",
    "import random\n",
    "import glob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Fix truncated images error\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# ==========================================\n",
    "# 1. UTILITIES & CLASSES\n",
    "# ==========================================\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=2):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "    def __len__(self): return len(self.itos)\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = collections.Counter()\n",
    "        idx = 4\n",
    "        for sentence in sentence_list:\n",
    "            for word in word_tokenize(str(sentence).lower()):\n",
    "                frequencies[word] += 1\n",
    "        for word, count in frequencies.items():\n",
    "            if count >= self.freq_threshold:\n",
    "                self.stoi[word] = idx; self.itos[idx] = word; idx += 1\n",
    "    def numericalize(self, text):\n",
    "        return [self.stoi.get(t, 3) for t in word_tokenize(str(text).lower())]\n",
    "\n",
    "def load_glove_embeddings(vocab, glove_path):\n",
    "    print(f\"üîå Loading GloVe from: {glove_path}\")\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
    "    matrix = np.zeros((len(vocab), 300))\n",
    "    hits = 0\n",
    "    for word, i in vocab.stoi.items():\n",
    "        v = embeddings_index.get(word)\n",
    "        if v is not None: matrix[i] = v; hits += 1\n",
    "    print(f\"   ‚úÖ GloVe Loaded: {hits} words matched.\")\n",
    "    return torch.tensor(matrix, dtype=torch.float32)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = self.bce(inputs, targets)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class MMHSDataset(Dataset):\n",
    "    def __init__(self, json_path, img_dir, vocab, transform=None, limit=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        if json_path and os.path.exists(json_path):\n",
    "            with open(json_path, 'r') as f: raw_data = json.load(f)\n",
    "            all_items = list(raw_data.items())\n",
    "            if limit:\n",
    "                random.shuffle(all_items)\n",
    "                all_items = all_items[:limit]\n",
    "            for k, v in all_items:\n",
    "                labels = v.get('labels', [])\n",
    "                if not labels: continue\n",
    "                label = 1 if sum(labels) >= 2 else 0\n",
    "                img_name = f\"{k}.jpg\"\n",
    "                if os.path.exists(os.path.join(img_dir, img_name)):\n",
    "                    self.data.append((img_name, v.get('tweet_text', \"\"), label))\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, text, label = self.data[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        try: image = Image.open(img_path).convert(\"RGB\")\n",
    "        except: image = Image.new('RGB', (224, 224))\n",
    "        if self.transform: image = self.transform(image)\n",
    "        tokens = self.vocab.numericalize(text)\n",
    "        tokens = (tokens + [0]*60)[:60]\n",
    "        return image, torch.tensor(tokens, dtype=torch.long), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "class FacebookDataset(Dataset):\n",
    "    def __init__(self, json_path, img_dir, vocab, transform=None):\n",
    "        self.df = pd.read_json(json_path, lines=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row['img'])\n",
    "        try: image = Image.open(img_path).convert(\"RGB\")\n",
    "        except: image = Image.new('RGB', (224, 224))\n",
    "        if self.transform: image = self.transform(image)\n",
    "        tokens = self.vocab.numericalize(row['text'])\n",
    "        tokens = (tokens + [0]*60)[:60]\n",
    "        return image, torch.tensor(tokens, dtype=torch.long), torch.tensor(row['label'], dtype=torch.float32)\n",
    "\n",
    "# ==========================================\n",
    "# 2. MODEL\n",
    "# ==========================================\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        full_layers = list(resnet.children())[:-1] \n",
    "        self.backbone = nn.Sequential(*full_layers)\n",
    "        count = 0\n",
    "        for param in self.backbone.parameters():\n",
    "            if count < 100: param.requires_grad = False\n",
    "            else: param.requires_grad = True\n",
    "            count += 1\n",
    "        self.fc = nn.Sequential(nn.Flatten(), nn.Linear(2048, 512), nn.BatchNorm1d(512), nn.ReLU())\n",
    "    def forward(self, x): return self.fc(self.backbone(x))\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_weights):\n",
    "        super().__init__()\n",
    "        if embed_weights is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(embed_weights, freeze=False, padding_idx=0)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, 300, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(300, 256, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(512, 512)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x):\n",
    "        _, (h, _) = self.lstm(self.embedding(x))\n",
    "        return self.dropout(self.fc(torch.cat((h[-2], h[-1]), dim=1)))\n",
    "\n",
    "class TrojanModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_weights):\n",
    "        super().__init__()\n",
    "        self.vis = VisualEncoder()\n",
    "        self.txt = TextEncoder(vocab_size, embed_weights)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(1024, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.4),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    def forward(self, img, txt):\n",
    "        return self.head(torch.cat((self.vis(img), self.txt(txt)), dim=1))\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN PIPELINE\n",
    "# ==========================================\n",
    "def run_kaggle_training():\n",
    "    print(\"üöÄ INITIALIZING TRAINING PIPELINE...\")\n",
    "    \n",
    "    # --- AUTO-DETECT PATHS INSIDE FUNCTION ---\n",
    "    def find_file(filename, search_path):\n",
    "        for root, dirs, files in os.walk(search_path):\n",
    "            if filename in files: return os.path.join(root, filename)\n",
    "        return None\n",
    "\n",
    "    DATA_ROOT = '/kaggle/input'\n",
    "    \n",
    "    # 1. FIND DATASETS\n",
    "    FB_TRAIN = find_file(\"train.jsonl\", DATA_ROOT)\n",
    "    if not FB_TRAIN: raise FileNotFoundError(\"‚ùå Could not find train.jsonl\")\n",
    "    FB_ROOT = os.path.dirname(FB_TRAIN)\n",
    "    FB_IMG_DIR = os.path.join(FB_ROOT, 'img')\n",
    "    if not os.path.exists(FB_IMG_DIR):\n",
    "        sample = find_file(\"01235.png\", FB_ROOT)\n",
    "        if sample: FB_IMG_DIR = os.path.dirname(sample)\n",
    "\n",
    "    MMHS_GT = find_file(\"MMHS150K_GT.json\", DATA_ROOT)\n",
    "    MMHS_IMG_DIR = None\n",
    "    if MMHS_GT:\n",
    "        MMHS_ROOT = os.path.dirname(MMHS_GT)\n",
    "        if os.path.exists(os.path.join(MMHS_ROOT, 'img_resized')):\n",
    "            MMHS_IMG_DIR = os.path.join(MMHS_ROOT, 'img_resized')\n",
    "        else:\n",
    "            MMHS_IMG_DIR = os.path.join(MMHS_ROOT, 'img')\n",
    "\n",
    "    # 2. FIND GLOVE (THE FIX IS HERE)\n",
    "    GLOVE_PATH = None\n",
    "    possible_glove = [\n",
    "        '/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.300d.txt',\n",
    "        '/kaggle/input/glove6b300dtxt/glove.6B.300d.txt',\n",
    "        'glove.6B.300d.txt'\n",
    "    ]\n",
    "    for p in possible_glove:\n",
    "        if os.path.exists(p): GLOVE_PATH = p; break\n",
    "    \n",
    "    if not GLOVE_PATH:\n",
    "        print(\"‚¨áÔ∏è Downloading GloVe (Fallback)...\")\n",
    "        os.system(\"wget -q http://nlp.stanford.edu/data/glove.6B.zip\")\n",
    "        os.system(\"unzip -q -o glove.6B.zip\")\n",
    "        GLOVE_PATH = 'glove.6B.300d.txt'\n",
    "\n",
    "    print(f\"   ‚úÖ Data Found: {FB_TRAIN}\")\n",
    "    print(f\"   ‚úÖ GloVe Found: {GLOVE_PATH}\")\n",
    "\n",
    "    # CONFIG\n",
    "    BATCH_SIZE = 128\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"   üöÄ Accelerator: {DEVICE} (GPUs: {torch.cuda.device_count()})\")\n",
    "\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "    # --- BUILD ---\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)), transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    print(\"üöß Building Vocabulary...\")\n",
    "    df_fb = pd.read_json(FB_TRAIN, lines=True)\n",
    "    vocab = Vocabulary()\n",
    "    vocab.build_vocabulary(df_fb['text'].tolist())\n",
    "    \n",
    "    glove = load_glove_embeddings(vocab, GLOVE_PATH)\n",
    "    model = TrojanModel(len(vocab), glove)\n",
    "    \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"‚ö° Dual GPU Activated.\")\n",
    "        model = nn.DataParallel(model)\n",
    "        \n",
    "    model = model.to(DEVICE)\n",
    "    criterion = FocalLoss()\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # --- STAGE 1 ---\n",
    "    if MMHS_GT and MMHS_IMG_DIR:\n",
    "        print(\"\\n=== STAGE 1: PRE-TRAINING (30k) ===\")\n",
    "        mmhs_ds = MMHSDataset(MMHS_GT, MMHS_IMG_DIR, vocab, transform, limit=30000)\n",
    "        mmhs_loader = DataLoader(mmhs_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        \n",
    "        model.train()\n",
    "        loop = tqdm(mmhs_loader)\n",
    "        for img, txt, lbl in loop:\n",
    "            img, txt, lbl = img.to(DEVICE), txt.to(DEVICE), lbl.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            with autocast():\n",
    "                loss = criterion(model(img, txt).squeeze(), lbl)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # --- STAGE 2 ---\n",
    "    print(\"\\n=== STAGE 2: FINE-TUNING ===\")\n",
    "    fb_train = FacebookDataset(FB_TRAIN, FB_IMG_DIR, vocab, transform)\n",
    "    dev_path = find_file(\"dev_seen.jsonl\", DATA_ROOT) or find_file(\"dev.jsonl\", DATA_ROOT)\n",
    "    fb_dev = FacebookDataset(dev_path, FB_IMG_DIR, vocab, transform)\n",
    "    \n",
    "    # Balancing\n",
    "    targets = fb_train.df['label'].values\n",
    "    weights = [1./len(targets[targets==0]), 1./len(targets[targets==1])]\n",
    "    samples_weight = [weights[int(t)] for t in targets]\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    \n",
    "    train_loader = DataLoader(fb_train, batch_size=BATCH_SIZE, sampler=sampler, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(fb_dev, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    opt = optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "    # FIX: verbose removed\n",
    "    sched = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=1)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        for img, txt, lbl in loop:\n",
    "            img, txt, lbl = img.to(DEVICE), txt.to(DEVICE), lbl.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            with autocast():\n",
    "                loss = criterion(model(img, txt).squeeze(), lbl)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "        # Eval\n",
    "        model.eval()\n",
    "        correct = 0; total = 0; val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for img, txt, lbl in val_loader:\n",
    "                img, txt, lbl = img.to(DEVICE), txt.to(DEVICE), lbl.to(DEVICE)\n",
    "                with autocast():\n",
    "                    out = model(img, txt).squeeze()\n",
    "                    val_loss += criterion(out, lbl).item()\n",
    "                preds = (torch.sigmoid(out) > 0.5).float()\n",
    "                correct += (preds == lbl).sum().item()\n",
    "                total += lbl.size(0)\n",
    "        \n",
    "        acc = 100 * correct / total\n",
    "        avg_val = val_loss/len(val_loader)\n",
    "        sched.step(avg_val)\n",
    "        print(f\"   Val Acc: {acc:.2f}% | Loss: {avg_val:.4f}\")\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.module.state_dict() if hasattr(model, 'module') else model.state_dict(), 'model_best.pth')\n",
    "            print(f\"   üíæ Saved: {acc:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_kaggle_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-08T11:06:10.720466Z",
     "iopub.status.busy": "2026-02-08T11:06:10.719806Z",
     "iopub.status.idle": "2026-02-08T11:18:16.155675Z",
     "shell.execute_reply": "2026-02-08T11:18:16.154943Z",
     "shell.execute_reply.started": "2026-02-08T11:06:10.720431Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è  Installing OpenAI CLIP & Dependencies (Requires Internet ON)...\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44.8/44.8 kB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: ftfy\n",
      "Successfully installed ftfy-6.3.1\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-0vzliwbd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-0vzliwbd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (26.0rc2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.23.0+cu126)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py): started\n",
      "  Building wheel for clip (setup.py): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=b00fa8809ff41ceb375249595c710709687fff403c731a06bb936ac867780375\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ciwzzfvx/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n",
      "üöÄ SYSTEM ONLINE: Running on cuda\n",
      "üîç Scanning for Datasets...\n",
      "   ‚úÖ Found Train Data: /kaggle/input/hatefulmemesproject/facebook/data/train.jsonl\n",
      "   ‚úÖ Found Image Dir: /kaggle/input/hatefulmemesproject/facebook/data/img\n",
      "üß† Loading CLIP ViT-B/32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338M/338M [00:03<00:00, 105MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading Datasets...\n",
      "\n",
      "üî• STARTING CLIP TRAINING PROTOCOL...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac7d81a775240ae971ae94770b74ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RESULTS: Acc: 58.80% | F1: 0.5402 | Loss: 1.0266\n",
      "   üíæ NEW RECORD! Model Saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad0bfb31e934f4e835fd4a9c9158b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RESULTS: Acc: 59.00% | F1: 0.5330 | Loss: 1.0546\n",
      "   üíæ NEW RECORD! Model Saved.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbc6b0bd7f648799a9158abd651ab28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RESULTS: Acc: 58.00% | F1: 0.5291 | Loss: 1.0673\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988690a6a03e414397fa51fe4ead63fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RESULTS: Acc: 57.60% | F1: 0.5160 | Loss: 1.0933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ecaa656bfd444fa92b33baa61a60e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RESULTS: Acc: 57.40% | F1: 0.5035 | Loss: 1.1226\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4a7153a4df4d1ca631834f89d78c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RESULTS: Acc: 57.80% | F1: 0.5012 | Loss: 1.1612\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4f9d29d0ce4c678e62bcd4ac388735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RESULTS: Acc: 58.20% | F1: 0.5059 | Loss: 1.1709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f619a0f6d464dc48367963773cd1b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RESULTS: Acc: 58.40% | F1: 0.5071 | Loss: 1.1714\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70be3efbe3d3489f85d9eb7ecdac930e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RESULTS: Acc: 58.80% | F1: 0.5142 | Loss: 1.1869\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833b7eba3b9e4fc3ade9306a88013f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RESULTS: Acc: 58.40% | F1: 0.5117 | Loss: 1.1886\n",
      "\n",
      "üèÜ Final Best Accuracy: 59.00%\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================\n",
    "#  üöÄ HATEFUL MEME DETECTION - THE \"NUCLEAR OPTION\" (OpenAI CLIP)\n",
    "#  Architecture: ViT-B/32 (Vision Transformer) | Target: >70% Accuracy Start\n",
    "# ==================================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# 1. AUTO-INSTALL DEPENDENCIES (Magic Fix)\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "try:\n",
    "    import clip\n",
    "    print(\"‚úÖ CLIP is already installed.\")\n",
    "except ImportError:\n",
    "    print(\"‚¨áÔ∏è  Installing OpenAI CLIP & Dependencies (Requires Internet ON)...\")\n",
    "    install(\"ftfy\")\n",
    "    install(\"regex\")\n",
    "    install(\"tqdm\")\n",
    "    install(\"git+https://github.com/openai/CLIP.git\")\n",
    "    import clip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Fix truncated images error\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# ==========================================\n",
    "# 2. CONFIGURATION\n",
    "# ==========================================\n",
    "CONFIG = {\n",
    "    'BATCH_SIZE': 64,       # Large batch for stable gradients\n",
    "    'EPOCHS': 10,           \n",
    "    'LR': 1e-4,             # Higher LR since we are only training the head\n",
    "    'DEVICE': \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    'MODEL_TYPE': \"ViT-B/32\", # The standard CLIP model\n",
    "    'POS_WEIGHT': 2.0       # Penalty for missing Hateful memes (Imbalance Fix)\n",
    "}\n",
    "\n",
    "print(f\"üöÄ SYSTEM ONLINE: Running on {CONFIG['DEVICE']}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. ROBUST DATA LOCATOR\n",
    "# ==========================================\n",
    "def find_file(filename, search_path):\n",
    "    for root, dirs, files in os.walk(search_path):\n",
    "        if filename in files:\n",
    "            return os.path.join(root, filename)\n",
    "    return None\n",
    "\n",
    "print(\"üîç Scanning for Datasets...\")\n",
    "DATA_ROOT = '/kaggle/input'\n",
    "\n",
    "# Find Train File\n",
    "FB_TRAIN = find_file(\"train.jsonl\", DATA_ROOT)\n",
    "if not FB_TRAIN:\n",
    "    raise FileNotFoundError(\"‚ùå CRITICAL: Could not find 'train.jsonl'. Did you add the dataset?\")\n",
    "\n",
    "# Find Image Directory (Smart Search)\n",
    "FB_ROOT = os.path.dirname(FB_TRAIN)\n",
    "FB_IMG_DIR = os.path.join(FB_ROOT, 'img')\n",
    "if not os.path.exists(FB_IMG_DIR):\n",
    "    # Fallback: Look for a known image\n",
    "    sample = find_file(\"01235.png\", FB_ROOT) # Common file in dataset\n",
    "    if sample: \n",
    "        FB_IMG_DIR = os.path.dirname(sample)\n",
    "    else:\n",
    "        # Fallback 2: Look in the parent directory\n",
    "        parent = os.path.dirname(FB_ROOT)\n",
    "        FB_IMG_DIR = os.path.join(parent, 'img')\n",
    "\n",
    "print(f\"   ‚úÖ Found Train Data: {FB_TRAIN}\")\n",
    "print(f\"   ‚úÖ Found Image Dir: {FB_IMG_DIR}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. DATASET CLASS (CLIP SPECIALIZED)\n",
    "# ==========================================\n",
    "class CLIPMemesDataset(Dataset):\n",
    "    def __init__(self, json_path, img_dir, preprocess):\n",
    "        self.df = pd.read_json(json_path, lines=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.preprocess = preprocess # CLIP's internal image cleaner\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # 1. Image\n",
    "        img_path = os.path.join(self.img_dir, row['img'])\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = self.preprocess(image) # Returns tensor (3, 224, 224)\n",
    "        except:\n",
    "            # Fallback for corrupt images\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "            image = self.preprocess(image)\n",
    "\n",
    "        # 2. Text (Tokenized by CLIP)\n",
    "        # Truncate to 77 tokens (CLIP limit)\n",
    "        text = clip.tokenize(str(row['text']), truncate=True).squeeze()\n",
    "        \n",
    "        # 3. Label\n",
    "        label = torch.tensor(row['label'], dtype=torch.float32)\n",
    "        \n",
    "        return image, text, label\n",
    "\n",
    "# ==========================================\n",
    "# 5. THE MODEL (FROZEN BACKBONE)\n",
    "# ==========================================\n",
    "class HatefulCLIPClassifier(nn.Module):\n",
    "    def __init__(self, model_type, device):\n",
    "        super().__init__()\n",
    "        print(f\"üß† Loading CLIP {model_type}...\")\n",
    "        self.clip_model, self.preprocess = clip.load(model_type, device=device, jit=False)\n",
    "        \n",
    "        # Convert to float32 (CLIP defaults to float16 which can cause NaN in training)\n",
    "        self.clip_model = self.clip_model.float()\n",
    "        \n",
    "        # FREEZE CLIP BACKBONE (Crucial for Stage 1)\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # The Classifier Head (Trainable)\n",
    "        # Input = 512 (Image) + 512 (Text) = 1024\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1) # Logits out\n",
    "        )\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        with torch.no_grad():\n",
    "            img_features = self.clip_model.encode_image(image)\n",
    "            txt_features = self.clip_model.encode_text(text)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat((img_features, txt_features), dim=1)\n",
    "        return self.classifier(combined.float())\n",
    "\n",
    "# ==========================================\n",
    "# 6. MAIN TRAINING LOOP\n",
    "# ==========================================\n",
    "def run_training():\n",
    "    # Setup\n",
    "    model_wrapper = HatefulCLIPClassifier(CONFIG['MODEL_TYPE'], CONFIG['DEVICE'])\n",
    "    model = model_wrapper.to(CONFIG['DEVICE'])\n",
    "    preprocess = model_wrapper.preprocess\n",
    "    \n",
    "    # Data Loaders\n",
    "    print(\"üì¶ Loading Datasets...\")\n",
    "    train_ds = CLIPMemesDataset(FB_TRAIN, FB_IMG_DIR, preprocess)\n",
    "    \n",
    "    # Find Dev/Val set\n",
    "    dev_path = find_file(\"dev_seen.jsonl\", DATA_ROOT) or find_file(\"dev.jsonl\", DATA_ROOT)\n",
    "    val_ds = CLIPMemesDataset(dev_path, FB_IMG_DIR, preprocess)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=CONFIG['BATCH_SIZE'], shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Weighted Loss (To fight the 64% Safe bias)\n",
    "    pos_weight = torch.tensor([CONFIG['POS_WEIGHT']]).to(CONFIG['DEVICE'])\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    # Optimizer (Only training the classifier head!)\n",
    "    optimizer = optim.AdamW(model.classifier.parameters(), lr=CONFIG['LR'], weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    print(\"\\nüî• STARTING CLIP TRAINING PROTOCOL...\")\n",
    "    \n",
    "    for epoch in range(CONFIG['EPOCHS']):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        for img, txt, lbl in loop:\n",
    "            img, txt, lbl = img.to(CONFIG['DEVICE']), txt.to(CONFIG['DEVICE']), lbl.to(CONFIG['DEVICE'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(img, txt).squeeze()\n",
    "            loss = criterion(output, lbl)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        preds_all = []\n",
    "        labels_all = []\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for img, txt, lbl in val_loader:\n",
    "                img, txt, lbl = img.to(CONFIG['DEVICE']), txt.to(CONFIG['DEVICE']), lbl.to(CONFIG['DEVICE'])\n",
    "                \n",
    "                out = model(img, txt).squeeze()\n",
    "                val_loss += criterion(out, lbl).item()\n",
    "                \n",
    "                # Sigmoid for probability\n",
    "                probs = torch.sigmoid(out)\n",
    "                preds = (probs > 0.5).float()\n",
    "                \n",
    "                preds_all.extend(preds.cpu().numpy())\n",
    "                labels_all.extend(lbl.cpu().numpy())\n",
    "                \n",
    "        # Metrics\n",
    "        acc = accuracy_score(labels_all, preds_all) * 100\n",
    "        f1 = f1_score(labels_all, preds_all)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        scheduler.step(acc)\n",
    "        \n",
    "        print(f\"   RESULTS: Acc: {acc:.2f}% | F1: {f1:.4f} | Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), 'best_clip_model.pth')\n",
    "            print(f\"   üíæ NEW RECORD! Model Saved.\")\n",
    "            \n",
    "    print(f\"\\nüèÜ Final Best Accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9441703,
     "sourceId": 14771110,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
